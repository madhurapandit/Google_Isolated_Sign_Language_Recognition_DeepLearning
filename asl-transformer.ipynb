{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport json\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential, Model \nfrom tensorflow.keras.layers import Dense, LSTM, Input, Masking, Activation, Dropout, Concatenate\nfrom tensorflow.keras.layers import Flatten, MultiHeadAttention, LayerNormalization\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom tensorflow.keras.optimizers.experimental import AdamW\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom keras.utils import to_categorical\nfrom keras.regularizers import l1, l2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-08T23:48:35.881564Z","iopub.execute_input":"2023-05-08T23:48:35.881965Z","iopub.status.idle":"2023-05-08T23:48:43.890911Z","shell.execute_reply.started":"2023-05-08T23:48:35.881914Z","shell.execute_reply":"2023-05-08T23:48:43.889546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mode = 'training'\n# mode = 'submission'\n# mode = 'inference_testing'","metadata":{"execution":{"iopub.status.busy":"2023-05-08T19:55:40.114515Z","iopub.execute_input":"2023-05-08T19:55:40.11503Z","iopub.status.idle":"2023-05-08T19:55:40.120618Z","shell.execute_reply.started":"2023-05-08T19:55:40.114988Z","shell.execute_reply":"2023-05-08T19:55:40.119194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## loading train.csv data\n\ntrain_df = pd.read_csv('/kaggle/input/asl-signs/train.csv')\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-08T19:55:40.751921Z","iopub.execute_input":"2023-05-08T19:55:40.752609Z","iopub.status.idle":"2023-05-08T19:55:41.008764Z","shell.execute_reply.started":"2023-05-08T19:55:40.752557Z","shell.execute_reply":"2023-05-08T19:55:41.007396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## loading sign index map \n\njson_file = open('/kaggle/input/asl-signs/sign_to_prediction_index_map.json')\nsign_label = json.load(json_file)  ## a dictionary variable with all labels and its integer representation\nn_signs = len(sign_label)\nn_signs  ## number of classes","metadata":{"execution":{"iopub.status.busy":"2023-05-08T04:40:29.801875Z","iopub.execute_input":"2023-05-08T04:40:29.802575Z","iopub.status.idle":"2023-05-08T04:40:29.814811Z","shell.execute_reply.started":"2023-05-08T04:40:29.802536Z","shell.execute_reply":"2023-05-08T04:40:29.813499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## understanding parquet data for one sign example given\n\npq_path = os.path.join('/kaggle/input/asl-signs',train_df['path'][0])   ## taking first data in df\npq_df = pd.read_parquet(pq_path)\npq_df","metadata":{"execution":{"iopub.status.busy":"2023-05-08T04:40:30.186909Z","iopub.execute_input":"2023-05-08T04:40:30.187608Z","iopub.status.idle":"2023-05-08T04:40:30.301845Z","shell.execute_reply.started":"2023-05-08T04:40:30.18757Z","shell.execute_reply":"2023-05-08T04:40:30.300644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pq_df[pq_df['frame']==20].groupby('type')['landmark_index'].count()  ## for one frame finding the landmark data","metadata":{"execution":{"iopub.status.busy":"2023-05-08T04:40:30.512576Z","iopub.execute_input":"2023-05-08T04:40:30.513188Z","iopub.status.idle":"2023-05-08T04:40:30.525245Z","shell.execute_reply.started":"2023-05-08T04:40:30.513145Z","shell.execute_reply":"2023-05-08T04:40:30.523858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Order is face, left_hand, pose, right_hand\n\nFor each type, the index starts from 0. Thus, should calculate for each type and get only required index\n\n0 - 468 --> face landmarks\n\n468 - 489 --> left hand landmarks\n\n489 - 522 --> pose landmarks\n\n522 - 543 --> right hand landmarks","metadata":{}},{"cell_type":"code","source":"del pq_df","metadata":{"execution":{"iopub.status.busy":"2023-05-08T04:40:31.91286Z","iopub.execute_input":"2023-05-08T04:40:31.913556Z","iopub.status.idle":"2023-05-08T04:40:31.921247Z","shell.execute_reply.started":"2023-05-08T04:40:31.913517Z","shell.execute_reply":"2023-05-08T04:40:31.920018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As seen, the most videos have between 1 and 300 frames, the counts of frames after 300 are very less almost 1 video per frame number only. But 50 is where max videos have frames \nHence, considering max_frames as 30. ","metadata":{}},{"cell_type":"markdown","source":"### Understanding landmarks in Mediapipeline holistic data","metadata":{}},{"cell_type":"code","source":"pip install mediapipe","metadata":{"execution":{"iopub.status.busy":"2023-05-08T04:40:33.647646Z","iopub.execute_input":"2023-05-08T04:40:33.648677Z","iopub.status.idle":"2023-05-08T04:40:46.386398Z","shell.execute_reply.started":"2023-05-08T04:40:33.648632Z","shell.execute_reply":"2023-05-08T04:40:46.38488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import mediapipe as mp\n\nlips_indices = set()\n\nfor ele in mp.solutions.face_mesh_connections.FACEMESH_LIPS:\n    lips_indices.add(ele[0])\n    lips_indices.add(ele[1])","metadata":{"execution":{"iopub.status.busy":"2023-05-08T04:40:46.391849Z","iopub.execute_input":"2023-05-08T04:40:46.392287Z","iopub.status.idle":"2023-05-08T04:40:47.024466Z","shell.execute_reply.started":"2023-05-08T04:40:46.392244Z","shell.execute_reply":"2023-05-08T04:40:47.023384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.array(lips_indices)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T04:40:47.029272Z","iopub.execute_input":"2023-05-08T04:40:47.031757Z","iopub.status.idle":"2023-05-08T04:40:47.043585Z","shell.execute_reply.started":"2023-05-08T04:40:47.031717Z","shell.execute_reply":"2023-05-08T04:40:47.042472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"left_hand_indices = [i for i in range(468,489)]\nright_hand_indices = [i for i in range(522,543)]\nnp.array(left_hand_indices), np.array(right_hand_indices)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T04:40:47.060175Z","iopub.execute_input":"2023-05-08T04:40:47.061449Z","iopub.status.idle":"2023-05-08T04:40:47.076253Z","shell.execute_reply.started":"2023-05-08T04:40:47.061413Z","shell.execute_reply":"2023-05-08T04:40:47.075236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In 0 - 468 of face landmarks, only the lip and outline indices are taken and only hand landmarks are taken. Pose is not taken. ","metadata":{}},{"cell_type":"code","source":"reqd_keypoints = list(lips_indices) + list(left_hand_indices) + list(right_hand_indices)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T04:40:47.078584Z","iopub.execute_input":"2023-05-08T04:40:47.079582Z","iopub.status.idle":"2023-05-08T04:40:47.087417Z","shell.execute_reply.started":"2023-05-08T04:40:47.079546Z","shell.execute_reply":"2023-05-08T04:40:47.08643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROWS_PER_FRAME = 543    ## number of landmark indexes for each frame\nMAX_LENGTH = 30\nN_KEYPTS = len(reqd_keypoints)  ## 82\nN_KEYPTS","metadata":{"execution":{"iopub.status.busy":"2023-05-08T04:40:47.089068Z","iopub.execute_input":"2023-05-08T04:40:47.090756Z","iopub.status.idle":"2023-05-08T04:40:47.105676Z","shell.execute_reply.started":"2023-05-08T04:40:47.090701Z","shell.execute_reply":"2023-05-08T04:40:47.104648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating Dataset for training and validation","metadata":{}},{"cell_type":"code","source":"## the way data will be loaded for evaluation, using same for training\ndef load_relevant_data_subset(pq_path):\n    data_columns = ['x', 'y', 'z']\n    data = pd.read_parquet(pq_path, columns=data_columns)\n    n_frames = int(len(data) / ROWS_PER_FRAME)\n    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n    return data.astype(np.float32)   ## (n_frames, ROWS_PER_FRAME, 3)  ## (n_frames, 543, 3)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T04:40:47.107399Z","iopub.execute_input":"2023-05-08T04:40:47.108109Z","iopub.status.idle":"2023-05-08T04:40:47.120777Z","shell.execute_reply.started":"2023-05-08T04:40:47.108074Z","shell.execute_reply":"2023-05-08T04:40:47.119584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_dataset(file_path_list, labels_list):\n    x_data = np.empty(shape=(len(file_path_list), MAX_LENGTH, N_KEYPTS*3)) \n    y_data = []\n\n    for i in tqdm(range(len(file_path_list))):\n        pq_path = os.path.join('/kaggle/input/asl-signs', file_path_list[i])\n        video_keypoints = load_relevant_data_subset(pq_path)  ## gives a numpy array of shape (n_frames, 543, 3)\n        video_keypoints = video_keypoints[:, reqd_keypoints]   ## keeping only required keypoints\n        video_keypoints[np.isnan(video_keypoints)] = 0        ## converting all nan to 0 in the numpy array\n           \n            \n        if video_keypoints.shape[0] < MAX_LENGTH:\n            diff = MAX_LENGTH - video_keypoints.shape[0]\n            video_keypoints = np.append(np.zeros((diff, N_KEYPTS, 3)), video_keypoints, axis=0)\n        else:\n            video_keypoints = video_keypoints[-(MAX_LENGTH):]\n            \n        x = np.concatenate([video_keypoints[...,i] for i in range(3)], -1)\n        \n        del video_keypoints\n        \n        x_data[i] = x\n\n        y_data.append(sign_label[labels_list[i]])\n        \n    x_data = np.asarray(x_data).astype(np.float32)\n    y_data = np.asarray(y_data).astype(np.int)\n        \n    \n    return x_data, y_data   ## this gives a x_data with max_length arrays","metadata":{"execution":{"iopub.status.busy":"2023-05-08T04:44:49.664937Z","iopub.execute_input":"2023-05-08T04:44:49.666009Z","iopub.status.idle":"2023-05-08T04:44:49.675665Z","shell.execute_reply.started":"2023-05-08T04:44:49.665934Z","shell.execute_reply":"2023-05-08T04:44:49.674467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The shape of data when giving input should be same, but here it will be different. Thus, will be using padding command to make the length equal for all arrays. ","metadata":{}},{"cell_type":"code","source":"## splitting data for training and validation  ## taking only 10,000 rows of data\nif mode == 'training':\n#     train_data, val_data = train_test_split(train_df.sample(10000), test_size=0.05, random_state=11)\n    train_data, val_data = train_test_split(train_df, test_size=0.05, random_state=21)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T04:44:51.353618Z","iopub.execute_input":"2023-05-08T04:44:51.35472Z","iopub.status.idle":"2023-05-08T04:44:51.375727Z","shell.execute_reply.started":"2023-05-08T04:44:51.354675Z","shell.execute_reply":"2023-05-08T04:44:51.374763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting training data in the form required for model \nif mode == 'training':\n\n    x_train, y_train = create_dataset(list(train_data['path']), list(train_data['sign']))","metadata":{"execution":{"iopub.status.busy":"2023-05-08T04:44:52.816152Z","iopub.execute_input":"2023-05-08T04:44:52.816572Z","iopub.status.idle":"2023-05-08T05:16:04.54405Z","shell.execute_reply.started":"2023-05-08T04:44:52.816536Z","shell.execute_reply":"2023-05-08T05:16:04.540625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting validation data in the form required for model \nif mode == 'training':\n\n    x_val, y_val = create_dataset(list(val_data['path']), list(val_data['sign']))","metadata":{"execution":{"iopub.status.busy":"2023-05-08T05:16:04.550375Z","iopub.execute_input":"2023-05-08T05:16:04.550692Z","iopub.status.idle":"2023-05-08T05:17:47.522059Z","shell.execute_reply.started":"2023-05-08T05:16:04.550662Z","shell.execute_reply":"2023-05-08T05:17:47.520649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.shape, y_train.shape, x_val.shape, y_val.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-08T05:17:47.524111Z","iopub.execute_input":"2023-05-08T05:17:47.524539Z","iopub.status.idle":"2023-05-08T05:17:47.533338Z","shell.execute_reply.started":"2023-05-08T05:17:47.524494Z","shell.execute_reply":"2023-05-08T05:17:47.53204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building Model and training","metadata":{}},{"cell_type":"code","source":"# # Define model input shape\ninput_shape = (None, N_KEYPTS*3)\n\n# Define input layer\ninputs = Input(shape=input_shape)\n\n# Define transformer layer\ntransformer_layer = MultiHeadAttention(num_heads=8, key_dim=64, dropout=0.3)\ntransformer_output = transformer_layer(inputs, inputs)\n\n# Add layer normalization and residual connection\ntransformer_output = LayerNormalization()(inputs + transformer_output)\n\n# Define LSTM layer\nlstm_output1 = LSTM(128, return_sequences=True)(transformer_output)\ndropout_output1 = Dropout(0.3)(lstm_output1)  ## dropout layer\n\n# Define LSTM layer\nlstm_output2 = LSTM(128)(dropout_output1)\ndropout_output2 = Dropout(0.3)(lstm_output2)  ## dropout layer\n\n## Adding dense layers \ndense_output = Dense(64)(dropout_output2)\ndense_output2 = Dense(32)(dense_output)\n\n# Define output layer\noutput = Dense(n_signs, activation='softmax')(dense_output2)\n\n# Create model\ntransformer_lstm_model = Model(inputs=inputs, outputs=output)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T05:17:47.54501Z","iopub.execute_input":"2023-05-08T05:17:47.54636Z","iopub.status.idle":"2023-05-08T05:17:51.548181Z","shell.execute_reply.started":"2023-05-08T05:17:47.546315Z","shell.execute_reply":"2023-05-08T05:17:51.547101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## compiling the model\n\ntransformer_lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy', 'sparse_top_k_categorical_accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-05-08T05:17:51.549517Z","iopub.execute_input":"2023-05-08T05:17:51.549884Z","iopub.status.idle":"2023-05-08T05:17:51.609121Z","shell.execute_reply.started":"2023-05-08T05:17:51.549843Z","shell.execute_reply":"2023-05-08T05:17:51.608117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## model summary\ntransformer_lstm_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-05-08T05:17:51.610435Z","iopub.execute_input":"2023-05-08T05:17:51.610808Z","iopub.status.idle":"2023-05-08T05:17:51.648565Z","shell.execute_reply.started":"2023-05-08T05:17:51.610769Z","shell.execute_reply":"2023-05-08T05:17:51.647781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating a folder to save model weights \n\n\nfolder_path ='/kaggle/working/Transformer_LSTM/'\n\nif os.path.exists(folder_path) == True:\n    None\nelse:\n    os.makedirs(folder_path)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T05:17:51.655409Z","iopub.execute_input":"2023-05-08T05:17:51.655911Z","iopub.status.idle":"2023-05-08T05:17:51.674232Z","shell.execute_reply.started":"2023-05-08T05:17:51.655873Z","shell.execute_reply":"2023-05-08T05:17:51.673347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# enforciong early stopping, saving weights and optimizing learning rate\n\ncheckpoint_list = [tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',patience=10, restore_best_weights=True), \n                   tf.keras.callbacks.ModelCheckpoint(filepath='/kaggle/working/Transformer_LSTM/weights_epoch_{epoch:02d}.hdf5', save_weights_only=True, monitor='val_accuracy', \n                                                      mode='max', save_best_only=True),\n                   tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_accuracy', factor = 0.2, patience = 5)]","metadata":{"execution":{"iopub.status.busy":"2023-05-08T05:17:51.678186Z","iopub.execute_input":"2023-05-08T05:17:51.678562Z","iopub.status.idle":"2023-05-08T05:17:51.688839Z","shell.execute_reply.started":"2023-05-08T05:17:51.678525Z","shell.execute_reply":"2023-05-08T05:17:51.687711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 20\nBATCH_SIZE = 256\nSTEPS_PER_EPOCH = len(train_data['path'])//BATCH_SIZE\nVAL_STEPS = len(val_data['path'])//BATCH_SIZE","metadata":{"execution":{"iopub.status.busy":"2023-05-08T05:17:51.69069Z","iopub.execute_input":"2023-05-08T05:17:51.691045Z","iopub.status.idle":"2023-05-08T05:17:51.699364Z","shell.execute_reply.started":"2023-05-08T05:17:51.69101Z","shell.execute_reply":"2023-05-08T05:17:51.698192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"STEPS_PER_EPOCH, VAL_STEPS","metadata":{"execution":{"iopub.status.busy":"2023-05-08T05:17:51.701111Z","iopub.execute_input":"2023-05-08T05:17:51.701958Z","iopub.status.idle":"2023-05-08T05:17:51.711051Z","shell.execute_reply.started":"2023-05-08T05:17:51.70192Z","shell.execute_reply":"2023-05-08T05:17:51.709897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 150\n\nif mode == 'training':\n    history = transformer_lstm_model.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, steps_per_epoch=STEPS_PER_EPOCH, \n                                     validation_data=(x_val, y_val), validation_steps=VAL_STEPS, callbacks=checkpoint_list)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T05:17:51.721184Z","iopub.execute_input":"2023-05-08T05:17:51.722082Z","iopub.status.idle":"2023-05-08T05:24:47.777628Z","shell.execute_reply.started":"2023-05-08T05:17:51.722042Z","shell.execute_reply":"2023-05-08T05:24:47.776496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if mode == \"training\":\n\n    fig,ax = plt.subplots(1, 2, figsize=(12, 6))\n    ax[0].plot(history.history['val_accuracy'], label='val_accuracy')\n    ax[0].plot(history.history['accuracy'], label='accuracy')\n    ax[0].set_xlabel('Epoch')\n    ax[0].set_ylabel('Accuracy')\n    ax[0].legend()\n    \n    ax[1].plot(history.history['val_loss'], label='val_loss')\n    ax[1].plot(history.history['loss'], label='loss')\n    ax[1].set_xlabel('Epoch')\n    ax[1].set_ylabel('Loss')\n    ax[1].legend()\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-08T05:25:16.888323Z","iopub.execute_input":"2023-05-08T05:25:16.889314Z","iopub.status.idle":"2023-05-08T05:25:17.26265Z","shell.execute_reply.started":"2023-05-08T05:25:16.889275Z","shell.execute_reply":"2023-05-08T05:25:17.260559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if mode == \"training\":\n    del x_train \n    del y_train\n    del x_val\n    del y_val","metadata":{"execution":{"iopub.status.busy":"2023-05-08T02:10:51.67833Z","iopub.status.idle":"2023-05-08T02:10:51.679271Z","shell.execute_reply.started":"2023-05-08T02:10:51.678994Z","shell.execute_reply":"2023-05-08T02:10:51.679027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Converting model to format as required by competition","metadata":{}},{"cell_type":"markdown","source":"#### Below code referenced from JESSE VAN DER LINDEN notebook for Google - Isolated Sign Language Recognition competition ","metadata":{}},{"cell_type":"code","source":"## the model input to the inference model while testing should be of shape (543, 3)\n## but the model that we have trained is takes input (537, 543*3)\n## thus adding a input layer before our trained model to change input shape as required for testing\n\ndef model_for_submission(model):\n    \n    input_layer = Input(shape=(ROWS_PER_FRAME, 3), name=\"inputs\")  ## added input layer\n    \n    ## keeping only required keypoints \n    processed_input = tf.gather(input_layer, reqd_keypoints, axis=1)\n    \n    ## if data has nan replacing that with 0\n    processed_input = tf.where(tf.math.is_nan(processed_input), tf.zeros_like(processed_input), processed_input)\n    \n    ## flatten x, y, z data\n    processed_input = tf.concat([processed_input[...,i] for i in range(3)], -1)\n    \n ## changing shape of array to (1, n_frames, N_KEYPTS*3)\n    processed_input = tf.expand_dims(processed_input,0)\n    \n    ## calling trained model\n    trained_model = model(processed_input)\n    \n    ## adding final layer \n    output_layer = Activation('linear', name='outputs')(trained_model)\n    \n    ## getting model\n    final_model = Model(inputs=input_layer, outputs=output_layer)\n    \n    ## compiling model\n    final_model.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return final_model","metadata":{"execution":{"iopub.status.busy":"2023-05-08T05:35:38.593096Z","iopub.execute_input":"2023-05-08T05:35:38.593841Z","iopub.status.idle":"2023-05-08T05:35:38.602237Z","shell.execute_reply.started":"2023-05-08T05:35:38.593802Z","shell.execute_reply":"2023-05-08T05:35:38.600913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mode = 'inference_testing'","metadata":{"execution":{"iopub.status.busy":"2023-05-08T05:35:40.832412Z","iopub.execute_input":"2023-05-08T05:35:40.832789Z","iopub.status.idle":"2023-05-08T05:35:40.838349Z","shell.execute_reply.started":"2023-05-08T05:35:40.832755Z","shell.execute_reply":"2023-05-08T05:35:40.837186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if mode=='inference_testing':\n    submission_model = model_for_submission(transformer_lstm_model)\n    submission_model.summary(expand_nested=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T05:35:41.659083Z","iopub.execute_input":"2023-05-08T05:35:41.659453Z","iopub.status.idle":"2023-05-08T05:35:42.139389Z","shell.execute_reply.started":"2023-05-08T05:35:41.65942Z","shell.execute_reply":"2023-05-08T05:35:42.138387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## saving the model in tflite\n\nif mode=='inference_testing':\n    converter = tf.lite.TFLiteConverter.from_keras_model(submission_model)\n    tflite_model = converter.convert()\n\n    with open('/kaggle/working/transformer_lstm_model.tflite','wb') as f:\n        f.write(tflite_model)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T05:35:45.397456Z","iopub.execute_input":"2023-05-08T05:35:45.398214Z","iopub.status.idle":"2023-05-08T05:35:58.539042Z","shell.execute_reply.started":"2023-05-08T05:35:45.398176Z","shell.execute_reply":"2023-05-08T05:35:58.537931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submitting the tflite file","metadata":{}},{"cell_type":"code","source":"mode = 'submission'","metadata":{"execution":{"iopub.status.busy":"2023-05-08T04:02:04.820592Z","iopub.execute_input":"2023-05-08T04:02:04.821399Z","iopub.status.idle":"2023-05-08T04:02:04.826472Z","shell.execute_reply.started":"2023-05-08T04:02:04.821358Z","shell.execute_reply":"2023-05-08T04:02:04.825212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if mode=='submission':\n    !zip submission.zip transformer_lstm_model.tflite","metadata":{"execution":{"iopub.status.busy":"2023-05-08T04:02:09.231097Z","iopub.execute_input":"2023-05-08T04:02:09.231486Z","iopub.status.idle":"2023-05-08T04:02:10.454952Z","shell.execute_reply.started":"2023-05-08T04:02:09.231453Z","shell.execute_reply":"2023-05-08T04:02:10.45368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Submission Code","metadata":{}},{"cell_type":"code","source":"mode='inference_testing'","metadata":{"execution":{"iopub.status.busy":"2023-05-08T04:02:25.534428Z","iopub.execute_input":"2023-05-08T04:02:25.535502Z","iopub.status.idle":"2023-05-08T04:02:25.541111Z","shell.execute_reply.started":"2023-05-08T04:02:25.535455Z","shell.execute_reply":"2023-05-08T04:02:25.539655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tflite-runtime==2.9.1","metadata":{"execution":{"iopub.status.busy":"2023-05-08T23:48:43.892535Z","iopub.execute_input":"2023-05-08T23:48:43.893366Z","iopub.status.idle":"2023-05-08T23:48:53.760662Z","shell.execute_reply.started":"2023-05-08T23:48:43.893321Z","shell.execute_reply":"2023-05-08T23:48:53.759295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## getting data as required in competition\nif mode=='inference_testing':\n    random_n = np.random.choice([i for i in range(len(val_data))], 100)\n#     print(random_n)\n    test_videos = []\n    test_labels = []\n    for n in random_n:\n        path = os.path.join('/kaggle/input/asl-signs',list(train_df['path'])[n])\n        test = load_relevant_data_subset(path)\n        test_videos.append(test)\n        label = list(train_df['sign'])[n]\n        test_labels.append(label)\n    \n    test_videos = np.array(test_videos)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def number_to_sign(number):\n    for key, value in sign_label.items():\n        if value == number:\n            return key","metadata":{"execution":{"iopub.status.busy":"2023-05-08T05:36:11.195836Z","iopub.execute_input":"2023-05-08T05:36:11.196269Z","iopub.status.idle":"2023-05-08T05:36:11.205153Z","shell.execute_reply.started":"2023-05-08T05:36:11.196226Z","shell.execute_reply":"2023-05-08T05:36:11.203767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n## using same inference code as given in compettiion\nimport tflite_runtime.interpreter as tflite\n\nif mode=='inference_testing':\n    interpreter = tflite.Interpreter('/kaggle/working/transformer_lstm_model.tflite')\n\n    found_signatures = list(interpreter.get_signature_list().keys())\n\n#     if REQUIRED_SIGNATURE not in found_signatures:\n#         raise KernelEvalException('Required input signature not found.')\n\n    prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n    \n    all_time = []\n    correct_count = 0\n    for i in range(len(test_videos)):\n        start_time = time.time()\n        output = prediction_fn(inputs=test_videos[i])\n        end_time = time.time()\n        inference_time = end_time - start_time\n        all_time.append(inference_time)\n        pred_n = np.argmax(output[\"outputs\"])\n        predicted_sign = number_to_sign(pred_n)\n        print(f\"True: {predicted_sign} \\t Precited: {list(train_df['sign'])[i]}\")\n        if predicted_sign == test_labels[i]:\n            correct_count += 1\n            \nprint(f'The number of signs predicted correctly were {correct_count} out of total {len(test_videos)}.')\nprint()\nprint(f'The Accuracy is {(correct_count/len(test_videos)):.2%}.')","metadata":{"execution":{"iopub.status.busy":"2023-05-08T05:39:16.651465Z","iopub.execute_input":"2023-05-08T05:39:16.651765Z","iopub.status.idle":"2023-05-08T05:39:21.147909Z","shell.execute_reply.started":"2023-05-08T05:39:16.651735Z","shell.execute_reply":"2023-05-08T05:39:21.146773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Average inference time: {np.mean(all_time):.4f}')\nmodel_size = os.path.getsize('/kaggle/working/transformer_lstm_model.tflite')\nprint(f'model size: {model_size/(1024**2):.3f} MB')","metadata":{"execution":{"iopub.status.busy":"2023-05-08T05:36:48.317661Z","iopub.execute_input":"2023-05-08T05:36:48.318076Z","iopub.status.idle":"2023-05-08T05:36:48.324474Z","shell.execute_reply.started":"2023-05-08T05:36:48.318039Z","shell.execute_reply":"2023-05-08T05:36:48.323249Z"},"trusted":true},"execution_count":null,"outputs":[]}]}